{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MYadav111/HateSpeech/blob/main/SBMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81fxr7OLwoaS"
      },
      "source": [
        "#Importing Data and Essential Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8liydqrvQlT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WvaIDRgsuWM",
        "outputId": "3a1d8e38-9f50-4f06-f8a8-7bf5f9786187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.util import pr\n",
        "import re\n",
        "import nltk\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "stopword=set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L1RMmlFsxX0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuKep-ioCOJ6"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from matplotlib import style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "-Zgn46-6ZDE5",
        "outputId": "75eef31e-375b-4717-8671-90dc81b31dcf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/labeled_data original.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-82731e62b939>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/labeled_data original.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/labeled_data original.csv'"
          ]
        }
      ],
      "source": [
        "tweet = pd.read_csv(\"/content/labeled_data original.csv\")\n",
        "print(tweet.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSYwldtOtmFr"
      },
      "outputs": [],
      "source": [
        "tweet.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR1iSugU2dLk"
      },
      "outputs": [],
      "source": [
        "#Mapping the Data\n",
        "tweet[\"labels\"] = tweet[\"class\"].map({0: \"Hate Speech\",\n",
        "                                    1: \"Offensive Language\",\n",
        "                                    2: \"No Hate and Offensive\"})\n",
        "print(tweet.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhBPfeGajCxE"
      },
      "outputs": [],
      "source": [
        "tweet['labels'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDMiN_27thHi"
      },
      "outputs": [],
      "source": [
        "tweet.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuTetvEHq6Tg"
      },
      "outputs": [],
      "source": [
        "#Removing Unwanted Columns\n",
        "tweet = tweet[[\"tweet\", \"labels\"]]\n",
        "print(tweet.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nufUriwhtwG6"
      },
      "source": [
        "#Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuFOlP63rOwr"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    text=\" \".join(text)\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    text=\" \".join(text)\n",
        "    return text\n",
        "tweet[\"tweet\"] = tweet[\"tweet\"].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBWADVEDtzR8"
      },
      "outputs": [],
      "source": [
        "tweet.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIb9chlt1Lj"
      },
      "source": [
        "#Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRf7H9KerXP7"
      },
      "outputs": [],
      "source": [
        "#Decision Tree Classifier\n",
        "x = np.array(tweet[\"tweet\"])\n",
        "y = np.array(tweet[\"labels\"])\n",
        "\n",
        "cv_DT = CountVectorizer()\n",
        "\n",
        "X = cv_DT.fit_transform(x) # Fit the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
        "\n",
        "clf_DT = DecisionTreeClassifier()\n",
        "clf_DT.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxtAc313sAjM"
      },
      "outputs": [],
      "source": [
        "y_pred_DT = clf_DT.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_DT)\n",
        "print(cm)\n",
        "\n",
        "acc_DT = accuracy_score(y_test, y_pred_DT)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Decision Tree Classifier is: \", acc_DT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hDkMSIIrkmK"
      },
      "outputs": [],
      "source": [
        "#Prediction using Decision Tree Classifier\n",
        "sample = \"Let's unite and kill all the people who are protesting against the government\"\n",
        "data = cv_DT.transform([sample]).toarray()\n",
        "print(clf_DT.predict(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs5486Kor3m3"
      },
      "outputs": [],
      "source": [
        "#Support Vector Classifier\n",
        "cv_SVC = CountVectorizer()\n",
        "\n",
        "X = cv_SVC.fit_transform(x) # Fit the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y, shuffle=True)\n",
        "\n",
        "clf_SVC = SVC()\n",
        "\n",
        "clf_SVC = SVC(kernel = 'linear', random_state = 0)\n",
        "clf_SVC.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RChTUJ-9r5Wt"
      },
      "outputs": [],
      "source": [
        "y_pred_SVC = clf_SVC.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_SVC)\n",
        "print(cm)\n",
        "\n",
        "acc_SVC = accuracy_score(y_test, y_pred_SVC)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Support Vector Classifier is: \", acc_SVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K4U66NQtW4Y"
      },
      "outputs": [],
      "source": [
        "#Prediction using Support Vector Classifier\n",
        "sample1 = \"Happy Everyone\"\n",
        "data1 = cv_SVC.transform([sample]).toarray()\n",
        "print(clf_SVC.predict(data1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "cv_NB = CountVectorizer()\n",
        "\n",
        "X = cv_NB.fit_transform(x) # Fit the Data\n",
        "X = X.toarray()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y, shuffle=True)\n",
        "\n",
        "clf_NB = GaussianNB()\n",
        "clf_NB.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "oIaJ1bAzi2QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_NB = clf_NB.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_NB)\n",
        "print(cm)\n",
        "\n",
        "acc_NB = accuracy_score(y_test, y_pred_NB)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Naive Bayes is: \", acc_NB)"
      ],
      "metadata": {
        "id": "L7W2RQfekoH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "cv_RF = CountVectorizer()\n",
        "\n",
        "X = cv_RF.fit_transform(x) # Fit the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y, shuffle=True)\n",
        "\n",
        "clf_RF = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "clf_RF.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2cl_p5UUk1s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_RF = clf_RF.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_RF)\n",
        "print(cm)\n",
        "\n",
        "acc_RF = accuracy_score(y_test, y_pred_RF)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Random Forest Classifier is: \", acc_RF)"
      ],
      "metadata": {
        "id": "XO9x8LZHlUuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Accuracy of Different ML Model"
      ],
      "metadata": {
        "id": "nDxAu3Tylf5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of Decision Tree Classifier is: \", acc_DT)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Random Tree Classifier is: \", acc_RF)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Support Vector Classifier is: \", acc_SVC)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Naive Bayes is: \", acc_NB)"
      ],
      "metadata": {
        "id": "1NW-IHjQlfe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_OIdWevwO7H"
      },
      "source": [
        "#Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_GOQHAQvp93"
      },
      "outputs": [],
      "source": [
        "style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NHGw2C6OBMb"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "sns.countplot(x='labels', data = tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXtzdA5DPmrz"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "colors = (\"red\", \"gold\", \"white\")\n",
        "wp = {'linewidth':2, 'edgecolor':\"black\"}\n",
        "tags = tweet['labels'].value_counts()\n",
        "explode = (0.1, 0.1, 0.1)\n",
        "tags.plot(kind='pie',autopct = '%1.1f%%', shadow=True, colors = colors, startangle =90,\n",
        "         wedgeprops = wp, explode = explode, label='')\n",
        "plt.title('Distribution of sentiments')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjUrz4HXQhPp"
      },
      "outputs": [],
      "source": [
        "non_hate_tweets = tweet[tweet.labels == \"No Hate and Offensive\"]\n",
        "non_hate_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfaq6M12QOoe"
      },
      "outputs": [],
      "source": [
        "text = ' '.join([word for word in non_hate_tweets['tweet']])\n",
        "plt.figure(figsize=(20,15), facecolor='None')\n",
        "wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most frequent words in non hate tweets', fontsize = 19)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR9QbkkAbBGd"
      },
      "outputs": [],
      "source": [
        "hate_tweets = tweet[tweet.labels == \"Hate Speech\"]\n",
        "hate_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsYZaIrabQsy"
      },
      "outputs": [],
      "source": [
        "text = ' '.join([word for word in hate_tweets['tweet']])\n",
        "plt.figure(figsize=(20,15), facecolor='None')\n",
        "wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most frequent words in hate tweets', fontsize = 19)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pklDQKyJbXSu"
      },
      "outputs": [],
      "source": [
        "offensive_tweets = tweet[tweet.labels == \"Offensive Language\"]\n",
        "offensive_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSTJ2oCabfkc"
      },
      "outputs": [],
      "source": [
        "text = ' '.join([word for word in offensive_tweets['tweet']])\n",
        "plt.figure(figsize=(20,15), facecolor='None')\n",
        "wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most frequent words in Offensive Language', fontsize = 19)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMghIxF3scsA"
      },
      "source": [
        "#Exporting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qr2y31kcnEt"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poLNg_DBcx_X"
      },
      "outputs": [],
      "source": [
        "filename = 'testsvc.sav'\n",
        "pickle.dump(clf_SVC, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmSZ1QqqdJvb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming x is a NumPy array\n",
        "x = np.array([[1,2]])\n",
        "\n",
        "column_names = ['id', 'label', 'tweet']\n",
        "for column_name, column in zip(column_names, x.T):\n",
        "    print(column_name, \":\", column)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}